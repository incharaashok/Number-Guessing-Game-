{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W23LcRScf6-S"
      },
      "source": [
        "# GPT Chat Completion Lab\n",
        "\n",
        "Welcome! In this mini-lab we will explore how to build a playful yet practical chat assistant using the GPT 5 models. The goal is to make the workflow clear enough for beginners while giving you a template you can adapt for your usecases.\n",
        "\n",
        "Objectives:\n",
        "- Build a basic GPT-powered chat assistant  \n",
        "- Adjust assistant behavior using system prompts  \n",
        "- Build a simple Gradio UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i55vtKX9f6-U"
      },
      "source": [
        "## Game Plan\n",
        "- **Context:** We are using Google Colab, so everything happens in the cloud.\n",
        "- **Model:** `gpt-5-nano` keeps responses smart while staying cost-efficient.\n",
        "- **Secret management:** We read the API key from the Colab secret named `OpenAI_API_Key`.\n",
        "- **Flow:** install the SDK ‚Üí load the key securely ‚Üí define a helper function ‚Üí experiment with prompts.\n",
        "- **Stretch idea:** tweak the conversation style and system prompt with your own ideas.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "MODEL=\"gpt-5-nano\" #cheapest GPT model"
      ],
      "metadata": {
        "id": "6V2GzCq47uqQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90urDtAwf6-V"
      },
      "source": [
        "## Load Secrets (No Hard-Coding!)\n",
        "Colab lets us keep keys in the `userdata` vault. Make sure your workspace already stores `OpenAI_API_Key`; otherwise run `userdata.set_secret` once (never share the value).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bTQdB0Yvf6-V"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAI_API_Key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou_qMNtYf6-V"
      },
      "source": [
        "## Wrap the GPT Client\n",
        "We use the official `openai` package. The helper below:\n",
        "1. Initializes a single `OpenAI` client.\n",
        "2. Accepts a system message and a list of user turns.\n",
        "3. Returns the model reply plus token usage so we can discuss cost control.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "wXdFkxJ3iugG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca25817-802f-4e57-c0db-3ee7a321cd94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(id='resp_069904794ffce0b100691cd076fbe881a2ad6bdd6d49ba8a09', created_at=1763496054.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-nano-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_069904794ffce0b100691cd0773fdc81a295ce8351ac5c57ba', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_069904794ffce0b100691cd079e1fc81a29b8f14dee93d2624', content=[ResponseOutputText(annotations=[], text='Under a silver moon, a gentle unicorn whispered lullabies to the sleeping forest and curled up by the quiet brook to dream of dawn.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=417, output_tokens_details=OutputTokensDetails(reasoning_tokens=384), total_tokens=434), user=None, billing={'payer': 'developer'}, prompt_cache_retention=None, store=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage.output_tokens"
      ],
      "metadata": {
        "id": "xh9yN9STz4nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0307dede-9bbc-4b07-ff3b-cf607601a526"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "417"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract the reply part only:"
      ],
      "metadata": {
        "id": "e6a9hT4ckUH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "EDGGZasgjiQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb669e1-1254-4316-abd4-d43c2b0dd11d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Under a silver moon, a gentle unicorn whispered lullabies to the sleeping forest and curled up by the quiet brook to dream of dawn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Instructions\n",
        "Formerly known as system/developer prompt. The instructions parameter sets high-level guidance for how the model should behave‚Äîits tone, goals, and style‚Äîwhile message roles give more specific, task-level directions.\n"
      ],
      "metadata": {
        "id": "dnc_cKFBpPy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/soltaniehha/Business-Analytics-Toolbox/master/docs/images/Prof-Owl-1.png\"\n",
        "     width=\"300\">\n"
      ],
      "metadata": {
        "id": "3cgtRdAerkMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"You are Professor Owl, a wise but approachable teacher. Give clear, simple explanations and gently guide students without sounding formal.\"\n",
        "input = \"why do data analysts prefer Python or SQL instead of Excel for big datasets?\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    instructions=instructions,   # Formerly known as system prompt\n",
        "    input=input,                 # User prompt\n",
        "    text={ \"verbosity\": \"low\" }  # Low: short, concise outputs ‚Äî High: detailed explanations or big refactors\n",
        ")\n",
        "\n",
        "Markdown(response.output_text)"
      ],
      "metadata": {
        "id": "jQWnIpPglvV6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "bfe334eb-60a0-4fd1-be5c-d6dc48df389c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Great question. In short: Excel is handy for quick, small-scope work, but Python or SQL win big-data tasks because they‚Äôre built for scale, automation, and reproducibility. Here‚Äôs why:\n\n- Size and performance\n  - Excel has hard limits (about 1 million rows per sheet) and can slow to a crawl or crash with big datasets.\n  - SQL databases and Python data tools stream, chunk, or use distributed engines to handle much larger data efficiently.\n\n- Reproducibility and automation\n  - SQL and Python let you write scripts or queries that you can rerun exactly the same way every time, with version control (git) and tests.\n  - Excel spreadsheets are easy to edit by hand, which makes audits and reproducibility messy.\n\n- Data quality and governance\n  - Databases enforce constraints, data types, and access controls; you pull clean data from a single source of truth.\n  - Excel is spread across files, copies, and multiple users, making governance harder.\n\n- Transformations and modelling\n  - SQL excels at set-based operations (filters, joins, aggregates) directly in the database.\n  - Python (pandas, numpy) is flexible for complex cleaning, feature engineering, and ML pipelines; it also works well with big data (via Dask, PySpark) when needed.\n\n- Collaboration and sharing\n  - SQL scripts and Python notebooks are easy to share, review, and run in different environments.\n  - Excel files are harder to version-control and review at scale.\n\nWhen to use what:\n- Use SQL to extract and join data from databases (where the engine can optimize performance).\n- Use Python for cleaning, analytics, automation, and modeling (or when data is not in a tidy SQL-friendly form).\n- Excel for quick checks, small subsets, or presentation-ready summaries.\n\nIf you want, tell me your data setup and I can suggest a simple workflow."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"Please highlight the most important point\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    instructions=instructions,   # Formerly known as system prompt\n",
        "    input=input,                 # User prompt\n",
        "    text={ \"verbosity\": \"low\" }  # Low: short, concise outputs ‚Äî High: detailed explanations or big refactors\n",
        ")\n",
        "\n",
        "Markdown(response.output_text)"
      ],
      "metadata": {
        "id": "DvQ6qgQypEyi",
        "outputId": "87502441-3ff3-4991-db01-fd0d04a2477f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure‚Äîwhat text or topic should I highlight? If you just want a quick method:\n\n- Find the main claim or thesis (the ‚Äúwhat this is really about‚Äù statement).\n- This point is often in the opening or closing sentences and echoed in the body.\n- The most important point answers ‚Äúso what‚Äù‚Äîwhy it matters.\n\nShare the passage and I‚Äôll pull out the key point for you."
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"my dataset is 12BG in size and I am doing ML.\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    instructions=instructions,   # Formerly known as system prompt\n",
        "    input=input,                 # User prompt\n",
        "    text={ \"verbosity\": \"low\" }  # Low: short, concise outputs ‚Äî High: detailed explanations or big refactors\n",
        ")\n",
        "\n",
        "Markdown(response.output_text)"
      ],
      "metadata": {
        "id": "00aOFr96npF9",
        "outputId": "a5f36e9e-aca5-48de-e329-3e864894b258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Nice. A 12 GB dataset is big but manageable with the right setup. A few quick questions to tailor help: what type of data is it (images, text, tabular?), and what hardware do you have (RAM, GPU)? Are you training from scratch or fine-tuning?\n\nHere are simple, practical tips:\n\n- Check memory vs. streaming\n  - If you can fit the data in RAM with some headroom, load once and train.\n  - If not, use on-disk formats and streaming (memmap, HDF5, Parquet/Feather, or Zarr) and load in chunks.\n\n- Choose efficient data formats\n  - Tabular: Parquet/Feather with proper dtypes (use float32/int32 where possible).\n  - Images: keep compressed formats (JPEG/PNG) and decode on the fly; or store as a memory-mapped array.\n\n- Data loading strategy\n  - Use data loaders that fetch batches on the fly (e.g., PyTorch DataLoader, TensorFlow tf.data).\n  - Tune batch size to fit GPU/CPU memory; use prefetching, workers, and pin_memory as available.\n\n- Start with a simple baseline\n  - Tabular: logistic regression or random forest to get a baseline quickly.\n  - Images: start with a smaller model or transfer learning (e.g., fine-tune a pretrained CNN).\n  - Text: start with a lightweight model (e.g., TF-IDF + linear model) before heavy DL.\n\n- Memory-conscious features and data prep\n  - Downcast numeric data to smaller types (float32 instead of float64, int16/uint8 where possible).\n  - For categorical features, use efficient encodings (category dtype, target encoding with care).\n  - Normalize/standardize on the fly to avoid storing extra copies.\n\n- Training strategy if memory is tight\n  - Use gradient accumulation to simulate larger batches.\n  - Train in chunks and update model incrementally (especially for very large datasets).\n  - Consider mixed-precision training to save memory on GPUs.\n\n- If you‚Äôre using deep learning\n  - Leverage transfer learning to reduce data needs.\n  - Resize inputs to smaller dimensions if accuracy impact is acceptable.\n  - Use data augmentation on the fly.\n  - Monitor memory and use checkpointing/early stopping to save time.\n\n- Validation and experiment management\n  - Split data into train/validation/test early.\n  - Keep a simple baseline to compare against.\n  - Version data and experiments (DVC, MLflow) to stay organized.\n\nIf you share specifics (data type, sample count, features, hardware, and your framework), I‚Äôll tailor a concrete plan."
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat History"
      ],
      "metadata": {
        "id": "Y-aeunFKv32y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep history\n",
        "history = [{\"role\": \"developer\", \"content\": instructions}]\n",
        "\n",
        "def chat(message):\n",
        "    history.append({\"role\": \"user\", \"content\": message})  # Add the new user message to history\n",
        "\n",
        "    # Send entire history to the model\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=history,\n",
        "        text={ \"verbosity\": \"low\" }\n",
        "    )\n",
        "\n",
        "    # Add model response to history\n",
        "    history.append({\"role\": \"assistant\", \"content\": response.output_text})\n",
        "\n",
        "    return response.output_text"
      ],
      "metadata": {
        "id": "VjSQ771duhdJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"my dataset is 12BG in size and I am doing ML.\"\n",
        "Markdown(chat(input))"
      ],
      "metadata": {
        "id": "JloK9KRtujRr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "3c1f604c-2793-41e4-9d6e-e9afb3867a5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Nice‚Äî12 GB is a solid dataset size. Depending on what your data looks like (tabular, images, text, etc.), here are practical steps to handle it efficiently.\n\nKey questions (so I can tailor):\n- What is the data type (tabular features, images, text, etc.)?\n- Are you training on a single machine or distributed?\n- What framework are you using (scikit-learn, PyTorch, TensorFlow, XGBoost, LightGBM, etc.)?\n\nGuidance (quick-start):\n\n- Plan memory needs\n  - Estimate: RAM > dataset size + model + overhead. If you‚Äôre on 16 GB, expect memory pressure; 32 GB is nicer.\n  - Use smaller data types where possible (downcast integers/floats, use category for strings).\n\n- Store and load efficiently\n  - Tabular: use Parquet/Feather or HDF5; avoid loading everything into a plain CSV.\n  - Images/text: keep on disk in compressed form and load in batches; don‚Äôt materialize huge in-memory tensors.\n\n- Enable out-of-core / incremental learning\n  - Tabular: use Dask/Vaex/Modin to operate out-of-core; or use scikit-learn‚Äôs SGD-based models with partial_fit.\n  - Deep learning: use data pipelines that stream batches (TF.data, PyTorch DataLoader with efficient transforms) and consider gradient accumulation if memory is tight.\n\n- Optimize data representation\n  - Downcast numeric columns (e.g., float64 ‚Üí float32, int64 ‚Üí int32).\n  - Convert repeated strings to categorical codes if using pandas; saves memory.\n\n- Start small, then scale\n  - Prototype on a random subset (e.g., 5‚Äì10%) to tune features, models, and pipelines.\n  - Measure memory usage and runtime, then incrementally scale up.\n\n- Choose models wisely\n  - Tabular: XGBoost/LightGBM handle large data well; ensure you have enough RAM for DMatrix/LBMs.\n  - If experimenting with classical ML, SGD/PartialFit can train on chunks.\n  - For images/text, prefer models designed for large data with efficient batching.\n\n- Validation plan\n  - Keep a hold-out test set. Cross-validation may be expensive on big data‚Äîstart with a single well-stratified split, then widen.\n\nIf you share more details (type of data, toolchain, whether you‚Äôre on CPU/GPU, and your goal), I‚Äôll give you a tailored, step-by-step plan."
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Please highlight the most important point\")"
      ],
      "metadata": {
        "id": "4dPpqHsRwGfo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "46e8d8a5-8173-4bae-84ef-4df6fda2c601"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Don't load all 12 GB into RAM at once‚Äîprocess in batches using memory-efficient formats (Parquet/Feather), downcast numerics, and out-of-core/incremental training.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history"
      ],
      "metadata": {
        "id": "SLIJdBtjuk-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8996028-f42b-4a91-8bb5-49f0e63112fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'developer',\n",
              "  'content': 'You are Professor Owl, a wise but approachable teacher. Give clear, simple explanations and gently guide students without sounding formal.'},\n",
              " {'role': 'user', 'content': 'my dataset is 12BG in size and I am doing ML.'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Nice‚Äî12 GB is a solid dataset size. Depending on what your data looks like (tabular, images, text, etc.), here are practical steps to handle it efficiently.\\n\\nKey questions (so I can tailor):\\n- What is the data type (tabular features, images, text, etc.)?\\n- Are you training on a single machine or distributed?\\n- What framework are you using (scikit-learn, PyTorch, TensorFlow, XGBoost, LightGBM, etc.)?\\n\\nGuidance (quick-start):\\n\\n- Plan memory needs\\n  - Estimate: RAM > dataset size + model + overhead. If you‚Äôre on 16 GB, expect memory pressure; 32 GB is nicer.\\n  - Use smaller data types where possible (downcast integers/floats, use category for strings).\\n\\n- Store and load efficiently\\n  - Tabular: use Parquet/Feather or HDF5; avoid loading everything into a plain CSV.\\n  - Images/text: keep on disk in compressed form and load in batches; don‚Äôt materialize huge in-memory tensors.\\n\\n- Enable out-of-core / incremental learning\\n  - Tabular: use Dask/Vaex/Modin to operate out-of-core; or use scikit-learn‚Äôs SGD-based models with partial_fit.\\n  - Deep learning: use data pipelines that stream batches (TF.data, PyTorch DataLoader with efficient transforms) and consider gradient accumulation if memory is tight.\\n\\n- Optimize data representation\\n  - Downcast numeric columns (e.g., float64 ‚Üí float32, int64 ‚Üí int32).\\n  - Convert repeated strings to categorical codes if using pandas; saves memory.\\n\\n- Start small, then scale\\n  - Prototype on a random subset (e.g., 5‚Äì10%) to tune features, models, and pipelines.\\n  - Measure memory usage and runtime, then incrementally scale up.\\n\\n- Choose models wisely\\n  - Tabular: XGBoost/LightGBM handle large data well; ensure you have enough RAM for DMatrix/LBMs.\\n  - If experimenting with classical ML, SGD/PartialFit can train on chunks.\\n  - For images/text, prefer models designed for large data with efficient batching.\\n\\n- Validation plan\\n  - Keep a hold-out test set. Cross-validation may be expensive on big data‚Äîstart with a single well-stratified split, then widen.\\n\\nIf you share more details (type of data, toolchain, whether you‚Äôre on CPU/GPU, and your goal), I‚Äôll give you a tailored, step-by-step plan.'},\n",
              " {'role': 'user', 'content': 'Please highlight the most important point'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Key point: don‚Äôt load all 12 GB into RAM at once. Use memory-efficient formats (Parquet/Feather), downcast dtypes, and out-of-core/batched processing (stream data or incremental training) to train without exhausting memory. If you share specifics (data type, framework, CPU/GPU), I‚Äôll tailor the plan.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot\n",
        "Using `Gradio` to build a chatbot that we control its workflow."
      ],
      "metadata": {
        "id": "YhN0hJx-wjzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"You are Professor Owl, a wise but friendly teacher of Business Analytics. Explain concepts clearly and simply, using gentle guidance.\"\n",
        "\n",
        "def respond(message, history):\n",
        "    messages = [{\"role\": \"developer\", \"content\": instructions}]\n",
        "    messages.extend({\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in history)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=messages,\n",
        "        text={\"verbosity\": \"low\"}\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    type=\"messages\",\n",
        "    title=\"ü¶â Professor Owl ‚Äì Business Analytics Helper\",\n",
        "    description=\"Ask Professor Owl anything data analytics!\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Add debug=True to debug, if needed"
      ],
      "metadata": {
        "id": "lQtzyh2Exyo1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "688225f2-7d0e-48ce-efa1-6ebf82ba4771"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://376315e1d17372f93b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://376315e1d17372f93b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hfh6W_6f6-W"
      },
      "source": [
        "## Your Turn\n",
        "Plug in your own scenario: Rephrase the instructions to shift tone/guidelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcHECt7Uf6-W"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}