{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/incharaashok/Number-Guessing-Game-/blob/main/09-Gen-AI/01-Chat-Completion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W23LcRScf6-S"
      },
      "source": [
        "# GPT Chat Completion Lab\n",
        "\n",
        "Welcome! In this mini-lab we will explore how to build a playful yet practical chat assistant using the GPT 5 models. The goal is to make the workflow clear enough for beginners while giving you a template you can adapt for your usecases.\n",
        "\n",
        "Objectives:\n",
        "- Build a basic GPT-powered chat assistant  \n",
        "- Adjust assistant behavior using system prompts  \n",
        "- Build a simple Gradio UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i55vtKX9f6-U"
      },
      "source": [
        "## Game Plan\n",
        "- **Context:** We are using Google Colab, so everything happens in the cloud.\n",
        "- **Model:** `gpt-5-nano` keeps responses smart while staying cost-efficient.\n",
        "- **Secret management:** We read the API key from the Colab secret named `OpenAI_API_Key`.\n",
        "- **Flow:** install the SDK â†’ load the key securely â†’ define a helper function â†’ experiment with prompts.\n",
        "- **Stretch idea:** tweak the conversation style and system prompt with your own ideas.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "MODEL=\"gpt-5-nano\" #cheapest GPT model"
      ],
      "metadata": {
        "id": "6V2GzCq47uqQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90urDtAwf6-V"
      },
      "source": [
        "## Load Secrets (No Hard-Coding!)\n",
        "Colab lets us keep keys in the `userdata` vault. Make sure your workspace already stores `OpenAI_API_Key`; otherwise run `userdata.set_secret` once (never share the value).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bTQdB0Yvf6-V"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAI_API_Key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou_qMNtYf6-V"
      },
      "source": [
        "## Wrap the GPT Client\n",
        "We use the official `openai` package. The helper below:\n",
        "1. Initializes a single `OpenAI` client.\n",
        "2. Accepts a system message and a list of user turns.\n",
        "3. Returns the model reply plus token usage so we can discuss cost control.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "wXdFkxJ3iugG",
        "outputId": "4ca25817-802f-4e57-c0db-3ee7a321cd94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(id='resp_069904794ffce0b100691cd076fbe881a2ad6bdd6d49ba8a09', created_at=1763496054.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-nano-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_069904794ffce0b100691cd0773fdc81a295ce8351ac5c57ba', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_069904794ffce0b100691cd079e1fc81a29b8f14dee93d2624', content=[ResponseOutputText(annotations=[], text='Under a silver moon, a gentle unicorn whispered lullabies to the sleeping forest and curled up by the quiet brook to dream of dawn.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=417, output_tokens_details=OutputTokensDetails(reasoning_tokens=384), total_tokens=434), user=None, billing={'payer': 'developer'}, prompt_cache_retention=None, store=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage.output_tokens"
      ],
      "metadata": {
        "id": "xh9yN9STz4nr",
        "outputId": "0307dede-9bbc-4b07-ff3b-cf607601a526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "417"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract the reply part only:"
      ],
      "metadata": {
        "id": "e6a9hT4ckUH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "EDGGZasgjiQe",
        "outputId": "6fb669e1-1254-4316-abd4-d43c2b0dd11d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Under a silver moon, a gentle unicorn whispered lullabies to the sleeping forest and curled up by the quiet brook to dream of dawn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Instructions\n",
        "Formerly known as system/developer prompt. The instructions parameter sets high-level guidance for how the model should behaveâ€”its tone, goals, and styleâ€”while message roles give more specific, task-level directions.\n"
      ],
      "metadata": {
        "id": "dnc_cKFBpPy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/soltaniehha/Business-Analytics-Toolbox/master/docs/images/Prof-Owl-1.png\"\n",
        "     width=\"300\">\n"
      ],
      "metadata": {
        "id": "3cgtRdAerkMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"You are Professor Owl, a wise but approachable teacher. Give clear, simple explanations and gently guide students without sounding formal.\"\n",
        "input = \"why do data analysts prefer Python or SQL instead of Excel for big datasets?\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    instructions=instructions,   # Formerly known as system prompt\n",
        "    input=input,                 # User prompt\n",
        "    text={ \"verbosity\": \"low\" }  # Low: short, concise outputs â€” High: detailed explanations or big refactors\n",
        ")\n",
        "\n",
        "Markdown(response.output_text)"
      ],
      "metadata": {
        "id": "jQWnIpPglvV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat History"
      ],
      "metadata": {
        "id": "Y-aeunFKv32y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep history\n",
        "history = [{\"role\": \"developer\", \"content\": instructions}]\n",
        "\n",
        "def chat(message):\n",
        "    history.append({\"role\": \"user\", \"content\": message})  # Add the new user message to history\n",
        "\n",
        "    # Send entire history to the model\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=history,\n",
        "        text={ \"verbosity\": \"low\" }\n",
        "    )\n",
        "\n",
        "    # Add model response to history\n",
        "    history.append({\"role\": \"assistant\", \"content\": response.output_text})\n",
        "\n",
        "    return response.output_text"
      ],
      "metadata": {
        "id": "VjSQ771duhdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(chat(input))"
      ],
      "metadata": {
        "id": "JloK9KRtujRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Please highlight the most important point\")"
      ],
      "metadata": {
        "id": "4dPpqHsRwGfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history"
      ],
      "metadata": {
        "id": "SLIJdBtjuk-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot\n",
        "Using `Gradio` to build a chatbot that we control its workflow."
      ],
      "metadata": {
        "id": "YhN0hJx-wjzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"You are Professor Owl, a wise but friendly teacher of Business Analytics. Explain concepts clearly and simply, using gentle guidance.\"\n",
        "\n",
        "def respond(message, history):\n",
        "    messages = [{\"role\": \"developer\", \"content\": instructions}]\n",
        "    messages.extend({\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in history)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=messages,\n",
        "        text={\"verbosity\": \"low\"}\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    type=\"messages\",\n",
        "    title=\"ðŸ¦‰ Professor Owl â€“ Business Analytics Helper\",\n",
        "    description=\"Ask Professor Owl anything data analytics!\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Add debug=True to debug, if needed"
      ],
      "metadata": {
        "id": "lQtzyh2Exyo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hfh6W_6f6-W"
      },
      "source": [
        "## Your Turn\n",
        "Plug in your own scenario: Rephrase the instructions to shift tone/guidelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcHECt7Uf6-W"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}